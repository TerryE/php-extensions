LPC Quick-Start Braindump

This is a rapidly written braindump of how LPC (a Lite Progam Cache) currently works in the form of
a quick-start guide to start hacking on LPC. It assumes that you are already familiar with the
corresponding guide to APC, so doesn't include the intro sections on how to get, install, buld and
debug it.


1. How LPC is different to APC.

PHP installations are broadly divided into two categories. Accelerators such as APC effectively
address the first of these: application-optimised systems which host a single application or a set
of mutually trusted applications on a dedicated server or VM. LPC is designed to address the second
category: shared hosting environments where a hosting provider provides the infrastructure and a
managed service the hosts a (large number of) number of independent accounts.

In a shared hosting environment, UID based access control is used to enable account separation.
Individual web-requests which require PHP script execution are executed in an account-specific UID
process: either on a per-request PHP activation (using suEXEC or suPHP), or sometimes using
per-account persistent PHP threads (using FastGCI). LPC aims to generate a comrparable performance
acceleration to APC but at his very different scaling sweet-spot and infrastructure template.

So LPC discards a lot of the functional richness of APC to focus on delivering the core functional
requirement and that is provide an opcode cache which obviates the need to enumerate and load the
many script files which implement an application and also the compilation process. It discards: 

*   All user variable/data object caching

*	Persistent shared memory, with the necessary SMA-base memory management, lock management, SMA 
    update integrity (with a one-per-request script file database used instead)  

*   The ability to save and load caches from user files

*   The bulk of the API including the APC iterator class

*   Various tuning and reporting INI parameters

*   Lazy function and class loading.

These incidentally represent roughly two-thirds of the APC source code base and nearly all of the
material covered in the APC TECHNOTES, so the LPC code base is a lot smaller. 

2.  More on LPC differences from APC

When I started using a triage strategy with the APC code base, sentencing it into three categories: 

*   Stuff which is no longer relevant to LPC such as the SMA management and could be removed;

*   Stuff which needed rewriting cache and pool implementations;

*   And finally code modules where I adopted a minimum change strategy (essentially apc_main.c and
    apc_cache.c. However as I started to develop / debug the core of these I realised that because
    LPC removed so much functionality and change much of the remainder, that these modules were also
    becoming a complete rewrite. So I abandoned the minimum change policy; instead adopting my own
    preference for minimum coding subject to clarity and functionality constraints. I discuss
    examples in the remainder of this section. 

APC has a difficult to understand policy on error handling as in general hard errors result in
functions aborting with an error status or NULL pointer being returning up the call stack, only for
these to be sometimes ignored at the higher call levels. Because LPC adopts an update-on-close
strategy for its file-based opcode cache, there are no cache integrity issues associated application
failure and it can follow a simple stragey: lpc errors are treated as fatal, except when the upper
tiers of application logic have functional code paths and rules to handle such failure returns.
 
The APC concept of a context is also dropped as the destination pool defines the context in LPC:
copying code out goes from an exec pool to a serial one, and copying code in v.v. Also since the
pool is also thread-specific, it also contains the TSRMLS id (in the ZTS variant) allowing the
TSRMLS_DC (et al) macros to be dropped from those functions which include a pool argument, whilst
still retaining thread-safety. lpc.h defines equivalent macros to allow calls to wider the PHP /
Zend API to recover these. 

The APC concept of lazy function and class loading is also dropped. This was introduced in 2009 to
support mega-applications such as facebook (see http://www.serverphorums.com/read.php?7,11145). It
really isn't the design point for LPC and it produces marginal if any performance gains for medium
applications such as Wordpress or mediawiki which use sensible autoload strategies anyway.


3.  Opcode caching at 20,000 ft

The PHP Zend engine is a compile and go system which compiles down to an intermediate form that is
build largely on the PHP storage mechanisms (zvals, hashtables, etc) and its emalloc memory
management subsistem to a format which is then interpreted by the Zend RTS. On request completion,
the RTS runs a distructor to enumerate and destroy these intermediate forms for each function,
class, etc. 

LPC hooks into this compile subsystem and intercepts the compile request. If no copy of the module
exists in its cache then is calls the underlying PHP compiler to do its business, but then does a
deep copy of the compiled out to a serial format before passing control back to the execution system
to run the code. 

If a copy of the module does exist in its cache then it does another deep copy, this time from the
serial format back into the standard emalloc-based format, in effect recreating the compiler output
but without invoking the compiler, and again passing control back to the execution system to run the
code. That is LPC in a nutshell. 

The deep copy function is based heavily on the corresponding code from APC.  The cache and memory 
pool code are total rewrites, because the requirements here are just too different.

All allocation and management of memory resources in done through an abstraction interface known as
a memory pool, and memory pools come in two flavours: 

*   An Exec pool is stored in the internal form expected by the Zend RTS.

*   A Serial pool is stored as a contiguous set of allocation units, that can be copied out to a
    record in the cache file, and then on subsequent request be reread (into probably a different
	base address).  Hence serial pool storage must be Position Independent -- that is all internal
    pointers must be correctly offset to match new base address of the storage.   

	Once relocated, the copy routine can successfully walk the internal storage structures with the
	serial pool in read-only mode -- which is all that is needed to copy it into a target exec pool.
	However, if any of the code paths attempt to use the emalloc subsystem to reclaim any of the
	serial pool elements, then the outcome is a messy death. So a deep copy is essential to preserve
	the integrity of the RTS. Once the deep copy is completed then the serial pool can be deleted
	en-masse with a single efree.

OK, there are a few wrinkles, such as how to implement the PIC identification and implementing
the cache itself so the remaining sections discuss these in more detail.


4.  Pools and Caches

The LPC pool and cache implementations are a complete rewrite compared to APC. This is because the
requirements driving their design are very different.

The cache is a fairly simple wrapper around a CacheDB cache file, with the file index loaded into a
hashtable as part of request initiation and contains one record per compiled module in initial
creation order. As most scripts follow the same programmatic path (as far as module loading is
concerned) -- or at least one of a relatively small number of variants -- in practice the cache file
is then read essentially sequentailly during the execution process to load all remaining modules.
This is the only file accessed to load all scripts under default INI settings. 

The pool supports the exec and serial forms as described above.  Unlike APC, the implementation of
the allocators, etc., is entirely private to the pool module, so the upper levels cannot use
'backdoor' access to these and MUST call the pool functions.  Also as with the emalloc system upon
which it is based, any allocation failures / memory exhaustion throw fatal errors, so the upper
levels do not need to check for or handle potential zero pointer returns, which simplifies error 
coding. 

Special alloc variants pool_alloc_zval and pool_alloc_ht exist which use the fast allocator macros 
for zval and HashTable structures in the exec pool variants.


5.  Ensuring that serial pools can be relocated 

A serial pool is initially allocated in a set of bricks (typically only one). The deep copy process
tags any putative internal pointers, and during creation these are maintained in a HashTable. As
part of the unload process this bricks are logically concatenated and this HashTable is converted
into a relocation vector (typically one byte per pointer) which is appended to the serial pool,
before being gzip compressed and written to the DB. (This compression is only done once during
initial compilation.

On reload, the gzip expansion overhead is relatively cheap compared to the compilation load, but the
approximate 4x reduction in the DB filesize DOES benefit subsequent retrievals. The relocation
vector is also enumerated to fix all internal pointer addresses.

To simplify this pointer tagging, all pool allocation routines take a (void**) destination as an
input parameter and return the (void*) to the destination address (Each is are wrapped in a macro to
allow the first argument to be expressed in any valid lvalue format) . This enables most internal
pointer tagging required for serial pool form to be handled internally within the pool logic.
However there also is an explicit tag function to enable upper copy code to use the pattern: 

	pool_alloc(tmp, some_size);
	...
	dest = tmp;
	pool_tag_ptr(&dest);

These allocation macros also use source line forwarding to the underlying _emalloc allocators, so
that the memory leakage reports in debugging mode give meaningful source addresses rather than all
pointing to some line in the pool allocator routine.

6.  Deep copying opcode structures

In APC the cached pools are in shared memory and accessible as a read-only resource by the execution
environment.  To reduce memory footprint, separate "for_execution" variants of the copy code 
duplicate only the runtime-modifiable elements into a local "unpool", and some care is taken with
reference counting to ensure that the Zend engine does not try to reclaim shared pool content.

In LPC, all pool structures (including serial pools reloaded from the cache file) are in local
memory. A part-clone strategy is memory inefficient, involves additional complexity and provides
minimal runtime savings. LPC therefore adopts a simpler strategy of LPC loading each cached compile
by reading in the serial pool form from file, then copying it in its entirety to an exec pool for
execution before immediately destroying the serial pool. Because the serial pool format is destroyed
before execution (unlike APC), and all copies must be deep; so there isn't any need for separate
"for_execution" variants of these copy routines.   




----------------------------

6. Next up is lpc_main.c and lpc_cache.c which implement the meat of the
   cache logic.

   The lpc_main.c file mostly calls functions in lpc_sma.c to allocate memory
   and lpc_cache.c for actual cache manipulation.  
  
   After the shared memory segment is created and the caches are initialized,
   lpc_module_init() installs the my_compile_file() function overriding Zend's
   version.  I'll talk about my_compile_file() and the rest of lpc_compile.c
   in the next section.  For now I will stick with lpc_main.c and lpc_cache.c
   and talk about the actual caches.  A cache consists of a block of shared
   memory returned by lpc_sma_allocate() via lpc_sma_malloc().  You will 
   notice references to lpc_emalloc().  lpc_emalloc() is just a thin wrapper
   around PHP's own emalloc() function which allocates per-process memory from
   PHP's pool-based memory allocator.  Don't confuse lpc_emalloc() and 
   lpc_sma_malloc() as the first is per-process and the second is shared memory.

   The cache is stored in/described by this struct allocated locally using
   emalloc():

     struct lpc_cache_t {
         void* shmaddr;              /* process (local) address of shared cache */
         header_t* header;           /* cache header (stored in SHM) */
         slot_t** slots;             /* array of cache slots (stored in SHM) */
         int num_slots;              /* number of slots in cache */
         int gc_ttl;                 /* maximum time on GC list for a slot */
         int ttl;                    /* if slot is needed and entry's access time is older than this ttl, remove it */
     };

   Whenever you see functions that take a 'cache' argument, this is what they
   take.  And lpc_cache_create() returns a pointer to this populated struct.

   At the beginning of the cache we have a header.  Remember, we are down a level now
   from the sma stuff.  The sma stuff is the low-level shared-memory allocator which
   has its own header which is completely separate and invisible to lpc_cache.c.  
   As far as lpc_cache.c is concerned the block of memory it is working with could 
   have come from a call to malloc().

   The header looks like this:

     typedef struct header_t header_t;
     struct header_t {
         int num_hits;               /* total successful hits in cache */
         int num_misses;             /* total unsuccessful hits in cache */
         slot_t* deleted_list;       /* linked list of to-be-deleted slots */
     };

   Since this is at the start of the shared memory segment, these values are accessible
   across all the yapache processes and hence access to them has to be locked.

   After the header we have an array of slots.  The number of slots is user-defined
   through the lpc.num_slots ini hint.  Each slot is described by:

     typedef struct slot_t slot_t;
     struct slot_t {
         lpc_cache_key_t key;        /* slot key */
         lpc_cache_entry_t* value;   /* slot value */
         slot_t* next;               /* next slot in linked list */
         int num_hits;               /* number of hits to this bucket */
	 time_t creation_time;       /* time slot was initialized */
	 time_t deletion_time;       /* time slot was removed from cache */
	 time_t access_time;         /* time slot was last accessed */
     };

   The slot_t *next there is a linked list to other slots that happened to hash to the
   same array position.

   lpc_cache_insert() shows what happens on a new cache insert.

     slot = &cache->slots[hash(key) % cache->num_slots];

   cache->slots is our array of slots in the segment.  hash() is simply:

     static unsigned int hash(lpc_cache_key_t key)
     {
         return key.data.file.device + key.data.file.inode;
     }

   That is, we use the file's device and inode to uniquely identify it.  Initially
   we had used the file's full path, but getting that requires a realpath() call which
   is amazingly expensive since it has to stat each component of the path to resolve
   symlinks and get rid of relative path components.  By using the device+inode we
   can uniquely identify a file with a single stat.

   So, on an insert we find the array position in the slots array by hashing the device+inode.
   If there are currently no other slots there, we just create the slot and stick it into
   the array:

     *slot = make_slot(key, value, *slot, t)

   If there are other slots already at this position we walk the link list to get to
   the end.  Here is the loop:

     while (*slot) {
         if (key_equals((*slot)->key.data.file, key.data.file)) {
	     /* If existing slot for the same device+inode is different, remove it and insert the new version */
	     if ((*slot)->key.mtime != key.mtime) {
	         remove_slot(cache, slot);
	         break;
	     }
	     UNLOCK(cache);
	     return 0;
	 } else if(cache->ttl && (*slot)->access_time < (t - cache->ttl)) {
             remove_slot(cache, slot);
             continue;
         }
         slot = &(*slot)->next;
     }

   That first key_equals() check sees if we have an exact match meaning the file
   is already in the cache.  Since we try to find the file in the cache before doing
   an insert, this will generally only happen if another process managed to beat us
   to inserting it.  If we have a newer version of the file at this point we remove
   it an insert the new version.  If our version is not newer we just return without
   doing anything.

   While walking the linked list we also check to see if the cache has a TTL defined.
   If while walking the linked list we see a slot that has expired, we remove it
   since we are right there looking at it.  This is the only place we remove stale
   entries unless the shared memory segment fills up and we force a full expunge via
   lpc_cache_expunge().  lpc_cache_expunge() walks the entire slots array and walks
   down every linked list removing stale slots to free up room.  This is obviously
   slow and thus only happens when we have run out of room.

   lpc_cache_find() simply hashes and returns the entry if it is there.  If it is there
   but older than the mtime in the entry we are looking for, we delete the one that is
   there and return indicating we didn't find it.

   Next we need to understand what an actual cache entry looks like.  Have a look at
   lpc_cache.h for the structs.  I sort of glossed over the key part earlier saying
   that we just used the device+inode to find a hash slot.  It is actually a bit more
   complex than that because we have two kinds of caches.  We have the standard file
   cache containing opcode arrays, but we also have a user-controlled cache that the
   user can insert whatever they want into via lpc_store().  For the user cache we
   obviously don't have a device+inode.  The actual identifier is provided by the user
   as a char *.  So the key is actually a union that looks like this:

     typedef union _lpc_cache_key_data_t {
         struct {
             int device;             /* the filesystem device */
             int inode;              /* the filesystem inode */
         } file;
         struct {
             char *identifier;
         } user;
     } lpc_cache_key_data_t;

     struct lpc_cache_key_t {
         lpc_cache_key_data_t data;
         int mtime;                  /* the mtime of this cached entry */
     };   

   And we have two sets of functions to do inserts and finds.  lpc_cache_user_find() 
   and lpc_cache_user_insert() operate on the user cache.

   Ok, on to the actual cache entry.  Again, because we have two kinds of caches, we
   also have the corresponding two kinds of cache entries described by this union:

     typedef union _lpc_cache_entry_value_t {
         struct {
             char *filename;             /* absolute path to source file */
             zend_op_array* op_array;    /* op_array allocated in shared memory */
             lpc_function_t* functions;  /* array of lpc_function_t's */
             lpc_class_t* classes;       /* array of lpc_class_t's */
         } file;
         struct {
             char *info;
             zval *val;
             unsigned int ttl;
         } user;
     } lpc_cache_entry_value_t;

   And then the actual cache entry:

     struct lpc_cache_entry_t {
         lpc_cache_entry_value_t data;
         unsigned char type;
         int ref_count;
     };

   The user entry is pretty simple and not all that important for now.  I will
   concentrate on the file entries since that is what holds the actual compiled
   opcode arrays along with the functions and classes required by the executor.

   lpc_cache_make_file_entry() in lpc_cache.c shows how an entry is constructed.
   The main thing to understand here is that we need more than just the opcode
   array, we also need the functions and classes created by the compiler when it
   created the opcode array.  As far as the executor is concerned, it doesn't know
   that it isn't operating in normal mode being called right after the parse/compile
   phase, so we need to recreate everything so it looks exactly like it would at
   that point. 

7. my_compile_file() and lpc_compile.c

   my_compile_file() in lpc_main.c controls where we get the opcodes from.  If
   the user-specified filters exclude the file from being cached, then we just
   call the original compile function and return.  Otherwise we fetch the request
   time from Apache to avoid an extra syscall, create the key so we can look up
   the file in the cache.  If we find it we stick it on a local stack which we
   use at cleanup time to make sure we return everything back to normal after a 
   request and call cached_compile() which installs the functions and classes
   associated with the op_array in this entry and then copy the op_array down
   into our memory space for execution.

   If we didn't find the file in the cache, we need to compile it and insert it.
   To compile it we simply call the original compile function:

      op_array = old_compile_file(h, type TSRMLS_CC);

   To do the insert we need to copy the functions, classes and the opcode array
   the compile phase created into shared memory.  This all happens in lpc_compile.c
   in the lpc_copy_op_array(), lpc_copy_new_functions() and lpc_copy_new_classes()
   functions.  Then we make the file entry and do the insert.  Both of these
   operations were described in the previous section.  

8. The Optimizer
   
   The optimizer has been deprecated.

If you made it to the end of this, you should have a pretty good idea of where things are in
the code.  I skimmed over a lot of things, so plan on spending some time reading through the code.

